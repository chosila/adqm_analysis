import os
import json
import argparse
import awkward
import numpy

from autodqm_ml.utils import setup_logger
from autodqm_ml.utils import expand_path
from autodqm_ml.plotting.plot_tools import make_original_vs_reconstructed_plot, make_sse_plot, plot_roc_curve
from autodqm_ml.evaluation.roc_tools import calc_roc_and_unc, print_eff_table
from autodqm_ml.constants import kANOMALOUS, kGOOD

def parse_arguments():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--output_dir",
        help = "output directory to place files in",
        type = str,
        required = False,
        default = "output"
    )
    parser.add_argument(
        "--input_file",
        help = "input file (i.e. output from fetch_data.py) to use for training the ML algorithm",
        type = str,
        required = False,
        default = None
    )
    parser.add_argument(
        "--histograms",
        help = "csv list of histograms to assess", 
        type = str,
        required = True,
        default = None
    )
    parser.add_argument(
        "--algorithms",
        help = "csv list of algorithmn names to assess",
        type = str,
        required = False,
        default = None
    )
    parser.add_argument(
        "--n_runs",
        help = "number of runs to make original/reconstructed plots for",
        type = int,
        required = False,
        default = 3
    )
    parser.add_argument(
        "--runs",
        help = "csv list of runs to make original/reconstructed plots for",
        type = str,
        required = False,
        default = None
    )
    parser.add_argument(
        "--make_webpage",
        required=False,
        action="store_true",
        help="make a nicely browsable web page"
    ) 
    parser.add_argument(
        "--debug",
        help = "run logger in DEBUG mode (INFO is default)",
        required = False,
        action = "store_true"
    )

    return parser.parse_args()


def infer_algorithms(runs, histograms, algorithms):
    for histogram, info in histograms.items():
        for field in runs.fields:
            if field == histogram:
                histograms[histogram]["original"] = field
            elif histogram in field and "_score_" in field:
                algorithm = field.replace(histogram, "").replace("_score_", "")
                if algorithms is not None:
                    if algorithm not in algorithms:
                        continue

                if not algorithm in info["algorithms"].keys():
                    histograms[histogram]["algorithms"][algorithm] = { "score" : field }

                # Check if a reconstructed histogram also exists for algorithm
                reco = field.replace("score", "reco")
                if reco in runs.fields:
                    histograms[histogram]["algorithms"][algorithm]["reco"] = reco
                else:
                    histograms[histogram]["algorithms"][algorithm]["reco"] = None

    return histograms


def main(args):
    os.system("mkdir -p %s/" % args.output_dir)

    logger_mode = "DEBUG" if args.debug else "INFO"
    log_file = "%s/fetch_data_log_%s.txt" % (args.output_dir, "assess")
    logger = setup_logger(logger_mode, log_file)


    histograms = { x : {"algorithms" : {}} for x in args.histograms.split(",") }

    runs = awkward.from_parquet(args.input_file)

    if args.algorithms is not None:
        algorithms = args.algorithms.split(",")
    else:
        algorithms = None

    histograms = infer_algorithms(runs, histograms, algorithms)
    for h, info in histograms.items():
        logger.debug("[assess.py] For histogram '%s', found the following anomaly detection algorithms:" % (h))
        for a, a_info in info["algorithms"].items():
            logger.debug("\t Algorithm '%s' with score in field '%s' and reconstructed histogram in field '%s'" % (a, a_info["score"], str(a_info["reco"])))
    

    # Print out runs with N highest sse scores for each histogram
    N = 5
    for h, info in histograms.items():
        for algorithm, algorithm_info in info["algorithms"].items():
            runs_sorted = runs[awkward.argsort(runs[algorithm_info["score"]], ascending=False)]
            logger.info("[assess.py] For histogram '%s', algorithm '%s', the mean +/- std anomaly score is: %.2e +/- %.2e." % (h, algorithm, awkward.mean(runs[algorithm_info["score"]]), awkward.std(runs[algorithm_info["score"]])))
            #logger.info("[assess.py] For histogram '%s', algorithm '%s', the runs with the highest anomaly scores are: " % (h, algorithm)) 
            logger.info("\t The runs with the highest anomaly scores are:")
            for i in range(N):
                logger.info("\t Run number : %d, Anomaly Score : %.2e" % (runs_sorted.run_number[i], runs_sorted[algorithm_info["score"]][i]))

    # Histogram of sse for algorithms
    splits = {
            "train_label" : [("train", 0), ("test", 1)],
            "label" : [("anomalous", kANOMALOUS), ("good", kGOOD)]
    }
 
    for h, info in histograms.items():
        for split, split_info in splits.items():
            recos_by_label = { k : {} for k,v in info["algorithms"].items() }
            for name, id in split_info:
                runs_set = runs[runs[split] == id]
                if len(runs_set) == 0:
                    logger.warning("[assess.py] For histogram '%s', no runs belong to the set '%s', skipping making a histogram of SSE for this." % (h, name))
                    continue
                recos = {}
                for algorithm, algorithm_info in info["algorithms"].items():
                    recos[algorithm] = { "score" : runs_set[algorithm_info["score"]] }
                    recos_by_label[algorithm][name] = { "score" : runs_set[algorithm_info["score"]] }

                h_name = h.replace("/", "").replace(" ", "")
                save_name = args.output_dir + "/" + h_name + "_sse_%s_%s.pdf" % (split, name)
                make_sse_plot(h_name, recos, save_name)

            for algorithm, recos_alg in recos_by_label.items():
                if not recos_alg:
                    continue
                save_name = args.output_dir + "/" + h_name + "_sse_%s_%s.pdf" % (algorithm, split)
                make_sse_plot(h_name, recos_alg, save_name) 


    # ROC curves (if there are labeled runs)
    has_labeled_runs = True
    labeled_runs_cut = runs.run_number < 0 # dummy all False cut
    for name, id in splits["label"]:
        cut = runs.label == id
        labeled_runs_cut = labeled_runs_cut | cut
        runs_set = runs[cut]
        has_labeled_runs = has_labeled_runs and (len(runs_set) > 0)

    if has_labeled_runs:
        labeled_runs = runs[labeled_runs_cut]
        roc_results = {}
        for h, info in histograms.items():
            roc_results[h] = {}
            for algorithm, algorithm_info in info["algorithms"].items():
                pred = labeled_runs[algorithm_info["score"]]
                roc_results[h][algorithm] = calc_roc_and_unc(labeled_runs.label, pred)

            h_name = h.replace("/", "").replace(" ", "")
            save_name = args.output_dir + "/" + h_name + "_roc.pdf"
            plot_roc_curve(h_name, roc_results[h], save_name)
            plot_roc_curve(h_name, roc_results[h], save_name.replace(".pdf", "_log.pdf"), log = True)
            print_eff_table(h_name, roc_results[h])
            

    # Plots of original/reconstructed histograms
    if args.runs is None:
        random_runs = True
        selected_runs_idx = numpy.random.choice(len(runs), size=args.n_runs, replace=False)
        selected_runs = runs.run_number[selected_runs_idx]
        logger.debug("[assess.py] An explicit list of runs was not given, so we will make plots for %d randomly chosen runs: %s" % (args.n_runs, str(selected_runs)))
    else:
        random_runs = False
        selected_runs = [int(x) for x in args.runs.split(",")]
        selected_runs_idx = runs.run_number < 0 # dummy all False
        for run in selected_runs:
            selected_runs_idx = selected_runs_idx | (runs.run_number == run)
        logger.debug("[assess.py] Will make plots for the %d specified runs: %s" % (len(selected_runs), str(selected_runs)))

    runs_trim = runs[selected_runs_idx]
    for h, info in histograms.items():
        for i in range(len(runs_trim)):
            run = runs_trim[i]
            run_number = run.run_number
            original = run[info["original"]]
            recos = {}
            for algorithm, algorithm_info in info["algorithms"].items():
                if algorithm_info["reco"] is None:
                    continue
                recos[algorithm] = { "reco" : run[algorithm_info["reco"]], "score" : run[algorithm_info["score"]]}
            h_name = h.replace("/", "").replace(" ", "")
            save_name = args.output_dir + "/" + h_name + "_Run%d.pdf" % run_number
            make_original_vs_reconstructed_plot(h_name, original, recos, run_number, save_name) 

    logger.info("[assess.py] Plots written to directory '%s'." % (args.output_dir))

    if args.make_webpage:
        os.system("cp web/index.php %s" % args.output_dir)
        os.system("chmod 755 %s" % args.output_dir)
        os.system("chmod 755 %s/*" % args.output_dir)

if __name__ == "__main__":
    args = parse_arguments()
    main(args)
